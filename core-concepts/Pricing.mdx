Retab uses a credit-based pricing system for AI model usage. Different models have different credit costs based on their capabilities and performance characteristics.

## Credit price

`1 Credit = 0.01$`

## Model Pricing

| Model Family | Model Variant | Credits | Tier |
|--------------|---------------|---------|------|
| **GPT-4.1** | nano | 0.1 | Micro |
| | mini | 0.5 | Small |
| | base | 2.0 | Large |
| **Gemini 2.5** | flash-lite | 0.1 | Micro |
| | flash | 0.5 | Small |
| | pro | 2.0 | Large |
| **o3** | base | 5.0 | Reasoning |
| **Retab router** | auto-micro | 0.1 | Micro |
| | auto-small | 0.5 | Small |
| | auto-large | 2.0 | Large |


## Extraction API Pricing

This concerns the following endpoints:

- [`v1/documents/extract`](https://docs.retab.com/api-reference/documents/extract)
- [`v1/processors/{processor_id}/submit`](https://docs.retab.com/api-reference/processors/submit)
- [automation runs](https://docs.retab.com/core-concepts/automations)

### Pricing Formula

The total cost for an extract request is calculated as:

```
credits/page = n_consensus × model_credits
```

### Credit Tiers

- **0.1 credits**: Micro models (fastest, most efficient)
- **0.5 credits**: Small models (balanced performance)
- **2.0 credits**: Large models (highest capability)
- **5.0 credits**: Reasoning models (highest tier)


Where:
- **n_consensus**: Number of consensus runs (typically 1-5, depending on your accuracy requirements)
- **model_credits**: The credit cost of the specific model you're using (see table above)


### Examples

**Example 1: Text PDF extraction with GPT-4.1-Mini**
- Model usage: 1 consensus × 0.5 credits = 0.5 credits
- **Total: 0.5 credits**

**Example 2: Scanned document with Gemini-2.5-Pro (3 consensus)**
- Model usage: 3 consensus × 2.0 credits = 6.0 credits
- **Total: 6.5 credits**

**Example 3: JSON extraction with Auto-Micro**
- Model usage: 1 consensus × 0.1 credits = 0.1 credits  
- **Total: 0.1 credits**

**Example 4: Scanned invoice with Auto-Micro**
- Model usage: 1 consensus × 0.1 credits = 0.1 credits
- **Total: 0.6 credits**

### Model Selection Guide

**Choose Micro models (0.1 credits)** when:
- You need fast, efficient processing
- Working with simple extraction tasks
- Cost efficiency is the primary concern

**Choose Small models (0.5 credits)** when:
- You need balanced performance and cost
- Working with moderate complexity tasks
- Good balance of speed and capability

**Choose Large models (2.0+ credits)** when:
- You need maximum capability and accuracy
- Working with complex reasoning tasks
- Quality is more important than cost

**Choose Reasoning models (5.0+ credits)** when:
- You need advanced logical reasoning
- Working with complex problem-solving tasks
- Maximum intelligence is required

## Parsing API Pricing

This concerns the following endpoints:

- [`v1/documents/parse`](https://docs.retab.com/api-reference/documents/parse)
- [`v1/documents/create_messages`](https://docs.retab.com/api-reference/documents/create_messages)
- [`v1/documents/create_inputs`](https://docs.retab.com/api-reference/documents/create_inputs)

### Pricing Formula

The total cost for a parse request is calculated as:

```
credits/page = model_credits
```

The Parse API follows the same pricing structure as extraction:
- **0 credits**: For text-based documents
- **model_credits**: The credit cost of the specific model you're using (see table above)

### Examples

**Example 1: PDF parsing with GPT-4.1-Mini**
- Model usage: 0.5 credits
- **Total: 0.5 credits**

**Example 2: Scanned document with Gemini-flash-lite-2.5**
- Model usage: 0.1 credits
- **Total: 0.1 credits**

**Example 3: JSON parsing with Auto-Micro**
- Model usage: 0.0 credits
- **Total: 0.0 credits**

**Example 3: Text parsing with Auto-Small**
- Model usage: 0.0 credits
- **Total: 0.0 credits**





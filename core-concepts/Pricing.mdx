---

## title: Pricing

---

Retab uses a credit-based pricing system for AI model usage. Different models have different credit costs based on their capabilities and performance characteristics.

## Credit price

`1 Credit = 0.01$`

## Model Pricing

| Model Family | Model Variant | Credits | Tier |
|--------------|---------------|---------|------|
| **GPT-5** | nano | 0.2 | Micro |
| | mini | 1.0 | Small |
| | base | 3.0 | Large |
| **Gemini 2.5** | flash-lite | 0.2 | Micro |
| | flash | 1.0 | Small |
| | pro | 3.0 | Large |
| **Claude 4.0** | sonnet | 3.0 | Large |
| **Retab router** | auto-micro | 0.2 | Micro |
| | auto-small | 1.0 | Small |
| | auto-large | 3.0 | Large |


## Extraction API Pricing

This concerns the following endpoints:

- [`v1/documents/extract`](https://docs.retab.com/api-reference/documents/extract)
- [`v1/processors/{processor_id}/submit`](https://docs.retab.com/api-reference/processors/submit)
- [automation runs](https://docs.retab.com/core-concepts/automations)

### Pricing Formula

The total cost for an extract request is calculated as:

```
credits/page = n_consensus × model_credits
```

### Credit Tiers

- **0.2 credits**: Micro models (fastest, most efficient)
- **0.5 credits**: Small models (balanced performance)
- **3.0 credits**: Large models (highest capability)

Where:
- **n_consensus**: Number of consensus runs (typically 1-5, depending on your accuracy requirements)
- **model_credits**: The credit cost of the specific model you're using (see table above)


### Examples

**Example 1: Text PDF extraction with GPT-5-Mini**
- Model usage: 1 run × 1.0 credits = 1.0 credits
- **Total: 1.0 credits**

**Example 2: Scanned document with Gemini-2.5-Pro (3 consensus)**
- Model usage: 3 runs × 3.0 credits = 9.0 credits
- **Total: 6.0 credits**

**Example 3: JSON extraction with Auto-Micro**
- Model usage: 1 run × 0.2 credits = 0.2 credits  
- **Total: 0.2 credits**

**Example 4: Scanned invoice with Auto-Micro**
- Model usage: 1 run × 0.2 credits = 0.2 credits
- **Total: 0.2 credits**

### Model Selection Guide

**Choose Micro models (0.2 credit)** when:
- You need fast, efficient processing
- Working with simple extraction tasks
- Cost efficiency is the primary concern

**Choose Small models (1.0 credit)** when:
- You need balanced performance and cost
- Working with moderate complexity tasks
- Good balance of speed and capability

**Choose Large models (3.0 credits)** when:
- You need maximum capability and accuracy
- Working with complex reasoning tasks
- Quality is more important than cost


## Parsing API Pricing

This concerns the following endpoints:

- [`v1/documents/parse`](https://docs.retab.com/api-reference/documents/parse)
- [`v1/documents/create_messages`](https://docs.retab.com/api-reference/documents/create_messages)
- [`v1/documents/create_inputs`](https://docs.retab.com/api-reference/documents/create_inputs)

### Pricing Formula

The total cost for a parse request is calculated as:

```
credits/page = model_credits
```

The Parse API follows the same pricing structure as extraction:
- **0 credits**: For text-based documents
- **model_credits**: The credit cost of the specific model you're using (see table above)

### Examples

**Example 1: PDF parsing with GPT-5-Mini**
- Model usage: 1.0 credits
- **Total: 0.5 credit**

**Example 2: Scanned document with Gemini-2.5-Flash-Lite**
- Model usage: 0.2 credits
- **Total: 0.2 credit**

**Example 3: JSON parsing with Auto-Micro**
- Model usage: 0.0 credit
- **Total: 0.0 credit**

**Example 3: Text parsing with Auto-Small**
- Model usage: 0.0 credit
- **Total: 0.0 credit**

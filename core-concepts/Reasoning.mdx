---

## title: Reasoning

---

When extracting data, LLMs sometimes make errors with calculations, unit conversions, or complex logic. Reasoning helps by encouraging the LLM to "show its work" before providing the final answer.

Reasoning uses a special JSON Schema annotation to create auxiliary fields where the LLM can explain its thought process:

- **`X-ReasoningPrompt`** - Generates reasoning fields alongside data fields, helping LLMs provide step-by-step justification for extracted values in complex scenarios.

This approach improves accuracy while keeping your original schema structure intact.


## Reasoning Prompt

A `X-ReasoningPrompt` tag in the schema generates a reasoning field alongside the data field. This is particularly useful for calculations, unit conversions, or multi-step logic.

**Example: Temperature Unit Conversion**

```markdown Temperature_report.md
| Date        | Location | Temperature (°C)| Humidity | Conditions    |
|-------------|----------|-----------------|----------|---------------|
| 2024-01-15  | New York | 22.5            | 65       | Partly Cloudy |
```






Let's say we have a temperature report in Celsius, but our data model expects the temperature in Fahrenheit:

<CodeGroup>
```python Python {11-13}
from pydantic import BaseModel, Field
from datetime import date

# You can define the custom annotations in the `pydantic.Field` class using the `json_schema_extra` field.

class TemperatureReport(BaseModel):
    date: date
    location: str
    temperature: float = Field(...,
        description="temperature in Fahrenheit",
        json_schema_extra={
            "X-ReasoningPrompt": "If the temperature is given in Celsius, make the explicit computation to convert it to Fahrenheit. If the temperature is given in Fahrenheit, leave it as is.",
        }
    )
    humidity: float
    conditions: str

# If you need a json_schema, you can call TemperatureReport.model_json_schema()
```
```typescript Javascript {10-12}
import { z } from "zod";

// You can define custom annotations by extending the schema with openapi metadata

const TemperatureReportSchema = z.object({
  date: z.string().date(),
  location: z.string(),
  temperature: z.number()
    .describe("temperature in Fahrenheit")
    .openapi({
      "X-ReasoningPrompt": "If the temperature is given in Celsius, make the explicit computation to convert it to Fahrenheit. If the temperature is given in Fahrenheit, leave it as is."
    }),
  humidity: z.number(),
  conditions: z.string()
});

type TemperatureReport = z.infer<typeof TemperatureReportSchema>;

// Generate JSON schema with: TemperatureReportSchema.openapi()
```
```json json_schema.json {13}
{
  "properties": {
    "date": {
      "format": "date",
      "title": "Date",
      "type": "string"
    },
    "location": {
      "title": "Location",
      "type": "string"
    },
    "temperature": {
      "X-ReasoningPrompt": "If the temperature is given in Celsius, make the explicit computation to convert it to Fahrenheit. If the temperature is given in Fahrenheit, leave it as is.",
      "description": "temperature in Fahrenheit",
      "title": "Temperature",
      "type": "number"
    },
    "humidity": {
      "title": "Humidity",
      "type": "number"
    },
    "conditions": {
      "title": "Conditions",
      "type": "string"
    }
  },
  "required": [
    "date",
    "location",
    "temperature",
    "humidity",
    "conditions"
  ],
  "title": "TemperatureReport",
  "type": "object"
}
```

</CodeGroup>

Without reasoning, the LLM might incorrectly use 22.5°F instead of converting from Celsius. With reasoning, it produces:

<CodeGroup>
```json With reasoning
{
  "date": "2024-01-15",
  "location": "New York",
  "reasoning___temperature": "The temperature is given as 22.5°C. To convert to Fahrenheit: F = (C × 9/5) + 32. So: F = (22.5 × 9/5) + 32 = 72.5°F",
  "temperature": 72.5,
  "humidity": 65,
  "conditions": "Partly Cloudy"
}
```
```json Without reasoning
{
  "date": "2024-01-15",
  "location": "New York",
  "temperature": 22.5,
  "humidity": 65,
  "conditions": "Partly Cloudy"
}
```
</CodeGroup>


As you can see, the "reasoning___" fields help the LLM show its work while the final output follows your schema structure.



---


## Key Benefits

1. **Accuracy**: LLMs perform calculations more reliably when they can show their work
2. **Transparency**: You can see exactly how the LLM arrived at its answer
3. **Debugging**: Easy to identify where conversions or calculations went wrong
4. **Trust**: Users can verify the logic behind complex transformations

## Best Practices

- Use reasoning fields for any calculations, unit conversions, or multi-step logic
- Make the reasoning description specific to guide the LLM's thought process
- Place reasoning fields before the fields that depend on the reasoning
- Keep reasoning concise but complete enough to follow the logic


## Go further

- [Schema](https://docs.retab.com/core-concepts/Schema)
- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
